# ============================================================
# IRENE VOICE ASSISTANT v14.0.0 - MASTER CONFIGURATION
# ============================================================
# 
# This is the MASTER configuration file containing ALL possible
# configuration options for Irene Voice Assistant v14.
#
# Use this file as a comprehensive reference for all available
# settings. For production use, copy sections you need to your
# own configuration file.
#
# Environment variables are supported using ${VAR_NAME} syntax
# Set IRENE_ASSETS_ROOT to control where models/cache are stored
# ============================================================

# ============================================================
# CORE SETTINGS
# ============================================================
name = "Irene"                      # Assistant name
version = "14.0.0"                  # Configuration version
debug = false                       # Enable debug mode for detailed logging
log_level = "INFO"                  # Logging level: DEBUG, INFO, WARNING, ERROR, CRITICAL

# Language and locale settings
language = "en-US"                  # Primary language (en-US, ru-RU, etc.)
timezone = "UTC"                    # Timezone (UTC, America/New_York, Europe/Moscow, etc.)

# Runtime performance settings
max_concurrent_commands = 10        # Maximum concurrent command processing
command_timeout_seconds = 30.0     # Command execution timeout
context_timeout_minutes = 30       # Conversation context timeout

# ============================================================
# SYSTEM CAPABILITIES - Hardware & Services
# ============================================================
[system]
# Hardware capabilities - what your system supports
microphone_enabled = true           # Enable microphone hardware capability
audio_playback_enabled = true      # Enable audio playback hardware capability

# Service capabilities - what services to run
web_api_enabled = true             # Enable REST API and WebSocket server
web_port = 8000                    # Web API server port (1-65535)
# NOTE: Metrics configuration moved to [monitoring] section (Phase 5 unification)

# ============================================================
# INPUT SOURCES - Data Entry Points
# ============================================================
[inputs]
# Input source enablement
microphone = true                   # Enable microphone input source
web = true                         # Enable web interface input (REST/WebSocket)
cli = true                         # Enable command line input
default_input = "microphone"       # Default input: "microphone", "web", "cli"

# Microphone Input Configuration
[inputs.microphone_config]
enabled = true                      # Enable microphone input processing
device_id = 7                       # Audio device ID (ThinkPad USB-C Dock Audio: 44100 Hz, 1 channel)
sample_rate = 44100                # Audio sample rate: matches device 7 native rate
channels = 1                       # Audio channels: 1 (mono) - matches device 7
chunk_size = 4096                  # Audio buffer chunk size for stable processing
buffer_queue_size = 50             # Audio buffer queue size for handling processing delays
auto_resample = true               # Enable automatic resampling for component compatibility
resample_quality = "medium"        # Resampling quality: "fast", "medium", "high", "best"

# Web Input Configuration
[inputs.web_config]
enabled = true                      # Enable web-based input processing
websocket_enabled = true           # Enable WebSocket real-time communication
rest_api_enabled = true            # Enable REST API endpoints

# CLI Input Configuration
[inputs.cli_config]
enabled = true                      # Enable command line input processing
prompt_prefix = "irene> "           # CLI prompt prefix
history_enabled = true              # Enable command history

# ============================================================
# COMPONENTS - Processing Pipeline Components
# ============================================================
[components]
# Component enablement - controls which processing components are loaded at startup
tts = true                         # Text-to-speech component
asr = true                         # Automatic speech recognition component
audio = true                       # Audio playback component
llm = true                         # Large language model component (optional)
voice_trigger = false              # Wake word detection component
nlu = true                         # Natural language understanding component
text_processor = true              # Text processing pipeline component
intent_system = true               # Intent handling component (essential)
monitoring = true                  # Monitoring and metrics component (Phase 3 infrastructure)

# ============================================================
# TTS (TEXT-TO-SPEECH) COMPONENT CONFIGURATION
# ============================================================
[tts]
enabled = true                      # Enable TTS component
default_provider = "console"        # Default provider: "console", "elevenlabs", "silero", "pyttsx", "vosk"
fallback_providers = ["console"]    # Fallback providers in order

# Console TTS Provider (for testing/development)
[tts.providers.console]
enabled = true                      # Enable console text output
color_output = true                # Use colored console output
timing_simulation = true           # Simulate TTS timing delays
prefix = "TTS: "                   # Prefix for console output

# ElevenLabs TTS Provider (high-quality neural TTS)
[tts.providers.elevenlabs]
enabled = false                     # Enable ElevenLabs provider
api_key = "${ELEVENLABS_API_KEY}"  # API key from environment variable
voice_id = "21m00Tcm4TlvDq8ikWAM"  # Voice ID to use
model = "eleven_monolingual_v1"    # Model: "eleven_monolingual_v1", "eleven_multilingual_v1"
stability = 0.5                    # Voice stability (0.0-1.0)
similarity_boost = 0.5             # Voice similarity boost (0.0-1.0)

# Silero v4 TTS Provider (neural TTS, latest version)
[tts.providers.silero_v4]
enabled = false                     # Enable Silero v4 provider
default_speaker = "xenia"          # Speaker voice: "xenia", "aidar", "baya", "kseniya", "eugene", "random"
sample_rate = 48000                # Audio sample rate: 24000, 48000, 96000
torch_device = "cpu"               # PyTorch device: "cpu", "cuda"
preload_models = true              # Preload AI models during provider initialization for faster startup

# Silero v3 TTS Provider (legacy support)
[tts.providers.silero_v3]
enabled = false                     # Enable Silero v3 provider
default_speaker = "xenia"          # Default speaker voice
sample_rate = 24000                # Audio sample rate
torch_device = "cpu"               # PyTorch device: "cpu", "cuda"
put_accent = true                  # Enable accent marks in speech
put_yo = true                      # Use Ñ‘ character in speech
threads = 4                        # Number of processing threads
preload_models = true              # Preload AI models during provider initialization for faster startup

# Pyttsx TTS Provider (cross-platform system TTS)
[tts.providers.pyttsx]
enabled = false                     # Enable pyttsx provider
voice_id = 0                       # Voice ID (system-dependent)
voice_rate = 200                   # Speech rate (words per minute)
voice_volume = 1.0                 # Volume level (0.0-1.0)

# Vosk TTS Provider (experimental)
[tts.providers.vosk]
enabled = false                     # Enable Vosk TTS provider
default_language = "ru"            # Default language: "ru", "en", "de", "fr"
sample_rate = 22050                # Audio sample rate
voice_speed = 1.0                  # Voice speed multiplier (0.1-3.0)
preload_models = true              # Preload AI models during provider initialization for faster startup

# ============================================================
# AUDIO (PLAYBACK) COMPONENT CONFIGURATION
# ============================================================
[audio]
enabled = true                      # Enable audio component
default_provider = "console"        # Default provider: "console", "sounddevice", "audioplayer", "aplay"
fallback_providers = ["console"]    # Fallback providers in order
concurrent_playback = false         # Allow multiple audio streams simultaneously

# Console Audio Provider (for testing/development)
[audio.providers.console]
enabled = true                      # Enable console audio simulation
color_output = true                # Use colored console output
timing_simulation = false          # Simulate audio playback timing

# SoundDevice Audio Provider (cross-platform, recommended)
[audio.providers.sounddevice]
enabled = false                     # Enable sounddevice provider
device_id = -1                     # Audio device ID (-1 = default device)
sample_rate = 44100                # Audio sample rate
channels = 2                       # Number of audio channels

# AudioPlayer Provider (Python audioplayer library)
[audio.providers.audioplayer]
enabled = false                     # Enable audioplayer provider
volume = 0.8                       # Volume level (0.0-1.0)
fade_in = false                    # Enable fade-in effect
fade_out = true                    # Enable fade-out effect

# Aplay Provider (Linux ALSA)
[audio.providers.aplay]
enabled = false                     # Enable aplay provider (Linux only)
device = "default"                 # ALSA device name

# ============================================================
# ASR (AUTOMATIC SPEECH RECOGNITION) COMPONENT CONFIGURATION
# ============================================================
[asr]
enabled = true                      # Enable ASR component
default_provider = "vosk"          # Default provider: "whisper", "vosk", "google_cloud"
fallback_providers = ["whisper"]   # Fallback providers in order
sample_rate = 16000                # AUTHORITATIVE: Component sample rate (overrides provider preferences)
channels = 1                       # AUTHORITATIVE: Number of audio channels (1=mono, 2=stereo)
allow_resampling = true            # Enable automatic resampling when rates don't match
resample_quality = "high"          # Resampling quality for ASR: "fast", "medium", "high", "best"
strict_validation = true           # Fatal error on provider configuration conflicts

# Whisper ASR Provider (OpenAI Whisper, recommended)
[asr.providers.whisper]
enabled = false                    # Enable Whisper provider
model_size = "base"                # Model size: "tiny", "base", "small", "medium", "large"
device = "cpu"                     # Device: "cpu", "cuda"
default_language = "ru"            # Language (null = auto-detect, "en", "ru", etc.)
temperature = 0.0                  # Temperature for decoding (0.0-1.0)
no_speech_threshold = 0.6          # No speech detection threshold
preload_models = true              # Preload AI models during provider initialization for faster startup

# Vosk ASR Provider (offline, lighter models)
[asr.providers.vosk]
enabled = true                     # Enable Vosk provider
default_language = "ru"            # Default language: "ru", "en"
sample_rate = 16000                # Audio sample rate (must match ASR component configuration)
confidence_threshold = 0.7         # Minimum confidence for recognition
preload_models = true              # Preload AI models during provider initialization for faster startup

# Google Cloud Speech ASR Provider (cloud-based, high accuracy)
[asr.providers.google_cloud]
enabled = false                     # Enable Google Cloud provider
credentials_path = "${GOOGLE_APPLICATION_CREDENTIALS}"  # Path to service account JSON
project_id = "your-project-id"     # Google Cloud project ID
default_language = "en-US"         # Default language code
sample_rate_hertz = 16000          # Audio sample rate
encoding = "LINEAR16"              # Audio encoding format

# ============================================================
# LLM (LARGE LANGUAGE MODEL) COMPONENT CONFIGURATION
# ============================================================
[llm]
enabled = true                      # Enable LLM component
default_provider = "openai"         # Default provider: "openai", "anthropic", "vsegpt", "console"
fallback_providers = ["console"]    # Fallback providers in order

# OpenAI LLM Provider (GPT models)
[llm.providers.openai]
enabled = true                      # Enable OpenAI provider
api_key = "${OPENAI_API_KEY}"      # API key from environment variable
base_url = "https://api.openai.com/v1"  # API base URL
default_model = "gpt-4o"            # Model: "gpt-3.5-turbo", "gpt-4o", "gpt-4o-mini"
max_tokens = 150                   # Maximum response tokens
temperature = 0.3                  # Creativity level (0.0-2.0)
top_p = 1.0                       # Nucleus sampling parameter
frequency_penalty = 0.0            # Frequency penalty (-2.0-2.0)
presence_penalty = 0.0             # Presence penalty (-2.0-2.0)

# Anthropic LLM Provider (Claude models)
[llm.providers.anthropic]
enabled = false                     # Enable Anthropic provider
api_key = "${ANTHROPIC_API_KEY}"   # API key from environment variable
base_url = "https://api.anthropic.com"  # API base URL
default_model = "claude-3-haiku-20240307"  # Model variant
max_tokens = 150                   # Maximum response tokens
temperature = 0.3                  # Creativity level (0.0-1.0)

# VSE GPT Provider (custom endpoint)
[llm.providers.vsegpt]
enabled = false                     # Enable VSE GPT provider
api_key = "${VSEGPT_API_KEY}"      # API key from environment variable
base_url = "https://api.vsegpt.ru/v1"  # Custom API base URL
default_model = "gpt-3.5-turbo"    # Model to use
max_tokens = 150                   # Maximum response tokens
temperature = 0.3                  # Creativity level

# Console LLM Provider (for testing/development)
[llm.providers.console]
enabled = true                      # Enable console LLM simulation
response = "LLM response would appear here"  # Default response text

# ============================================================
# VOICE TRIGGER (WAKE WORD) COMPONENT CONFIGURATION
# ============================================================
[voice_trigger]
enabled = false                      # Enable voice trigger component
default_provider = "openwakeword"   # Default provider: "openwakeword", "porcupine", "microwakeword"
wake_words = ["jarvis", "alexa"]    # Wake words to detect - using supported OpenWakeWord models
confidence_threshold = 0.8          # Detection confidence threshold (0.0-1.0)
buffer_seconds = 1.0               # Audio buffer duration in seconds
timeout_seconds = 5.0              # Detection timeout in seconds
sample_rate = 16000                # AUTHORITATIVE: Component sample rate (configuration takes precedence)
channels = 1                       # AUTHORITATIVE: Number of audio channels (1=mono, 2=stereo)  
allow_resampling = true            # Enable resampling for voice triggers (with optimized algorithms)
resample_quality = "fast"          # Resampling quality optimized for low-latency: "fast", "medium", "high"
strict_validation = true           # Fatal error on provider requirement conflicts

# OpenWakeWord Provider (recommended, open source)
[voice_trigger.providers.openwakeword]
enabled = false                      # Enable OpenWakeWord provider
inference_framework = "tflite"     # Framework: "onnx", "tflite" - using tflite to match downloaded models
vad_threshold = 0.5                # Voice activity detection threshold
chunk_size = 1280                  # Audio chunk size (80ms at 16kHz)
preload_models = false             # Preload AI models during provider initialization for faster startup

# Porcupine Provider (Picovoice, commercial)
[voice_trigger.providers.porcupine]
enabled = false                     # Enable Porcupine provider
access_key = "${PICOVOICE_ACCESS_KEY}"  # Picovoice access key
keywords = ["jarvis"]              # Built-in keywords to use

# MicroWakeWord Provider (ESP32/embedded)
[voice_trigger.providers.microwakeword]
enabled = false                     # Enable MicroWakeWord provider
feature_buffer_size = 49           # Feature buffer size (49 * 10ms = 490ms)
detection_window_size = 3          # Detection window size (consecutive detections needed)
stride_duration_ms = 10            # Audio processing stride in milliseconds  
window_duration_ms = 30            # Audio analysis window duration
num_mfcc_features = 40             # Number of MFCC features for analysis
preload_models = false             # Preload AI models during provider initialization for faster startup

# ============================================================
# NLU (NATURAL LANGUAGE UNDERSTANDING) COMPONENT CONFIGURATION
# ============================================================
[nlu]
enabled = true                      # Enable NLU component
default_provider = "hybrid_keyword_matcher"  # Default provider
confidence_threshold = 0.7          # Global confidence threshold (0.0-1.0)
fallback_intent = "conversation.general"    # Fallback intent name

# Cascading configuration for multiple providers
provider_cascade_order = ["hybrid_keyword_matcher", "spacy_nlu"]  # Fast to slow order
max_cascade_attempts = 4           # Maximum cascade attempts
cascade_timeout_ms = 200           # Timeout per cascade attempt
cache_recognition_results = false   # Cache recognition results
cache_ttl_seconds = 300            # Cache TTL in seconds

# Language Detection Configuration
auto_detect_language = true         # Enable automatic language detection
language_detection_confidence_threshold = 0.8  # Language detection confidence threshold
persist_language_preference = true  # Persist language preference across conversation
supported_languages = ["ru", "en"] # Supported languages
default_language = "ru"            # Default language when detection fails

# Hybrid Keyword Matcher Provider (fast, pattern-based)
[nlu.providers.hybrid_keyword_matcher]
enabled = true                      # Enable keyword matcher
confidence_boost = 0.1             # Confidence boost for exact matches
case_sensitive = false             # Case-sensitive matching
word_boundaries = true             # Require word boundaries

# SpaCy NLU Provider (advanced linguistic analysis)
[nlu.providers.spacy_nlu]
enabled = true                     # Enable spaCy provider
model_name = "ru_core_news_sm"      # SpaCy model: "en_core_web_sm", "ru_core_news_sm"
fallback_model = "en_core_web_sm"   # Fallback model if primary fails
confidence_threshold = 0.6         # Provider-specific confidence threshold
enable_ner = true                  # Enable named entity recognition
enable_pos_tagging = true          # Enable part-of-speech tagging
auto_download = true               # Auto-download model if missing via spacy CLI

# ============================================================
# TEXT PROCESSOR COMPONENT CONFIGURATION
# ============================================================
[text_processor]
enabled = true                      # Enable text processing pipeline
stages = ["asr_output", "tts_input", "command_input", "general"]  # Processing stages

# Text normalizer configurations
[text_processor.normalizers.numbers]
enabled = true                      # Enable number normalization
stages = ["asr_output", "general", "tts_input"]  # Stages to apply to

[text_processor.normalizers.prepare]
enabled = true                      # Enable text preparation
stages = ["tts_input", "general"]   # Stages to apply to  
latin_to_cyrillic = true           # Convert Latin to Cyrillic
symbol_replacement = true          # Replace symbols with words

[text_processor.normalizers.runorm]
enabled = true                      # Enable Russian normalization (RuNorm)
stages = ["tts_input"]             # Stages to apply to
model_size = "small"               # Model size: "small", "medium", "large"
device = "cpu"                     # Device: "cpu", "cuda"

# Provider configurations
[text_processor.providers.asr_text_processor]
enabled = true                      # Enable ASR text processor
language = "ru"                    # Language for processing

[text_processor.providers.general_text_processor]
enabled = true                      # Enable general text processor
language = "ru"                    # Language for processing

[text_processor.providers.number_text_processor]
enabled = true                      # Enable number text processor
language = "ru"                    # Language for processing

[text_processor.providers.tts_text_processor]
enabled = false                     # Enable TTS text processor (resource-intensive)
language = "ru"                    # Language for processing

# ============================================================
# INTENT SYSTEM COMPONENT CONFIGURATION
# ============================================================
[intent_system]
enabled = true                      # Enable intent system component (essential)
confidence_threshold = 0.7          # Minimum confidence for intent recognition (0.0-1.0)
fallback_intent = "conversation.general"  # Fallback intent when recognition fails

# Intent handler configuration
[intent_system.handlers]
enabled = ["conversation", "greetings", "timer", "datetime", "system", "text_enhancement_handler"]  # Enabled handlers
disabled = ["train_schedule"]       # Disabled handlers
auto_discover = true               # Automatically discover handlers
discovery_paths = ["irene.intents.handlers"]  # Entry-point discovery paths

# Asset validation configuration
[intent_system.handlers.asset_validation]
strict_mode = true                 # Enable strict validation mode
validate_method_existence = true   # Validate handler methods exist
validate_spacy_patterns = false    # Validate spaCy patterns (disabled for runtime validation)
validate_json_schema = true        # Validate JSON schema compliance

# Specific handler configurations
[intent_system.handlers.conversation]
session_timeout = 1800             # Session timeout in seconds (30 minutes)
max_sessions = 50                  # Maximum concurrent sessions
max_context_length = 10            # Maximum conversation context length
default_conversation_confidence = 0.6  # Default confidence threshold

[intent_system.handlers.train_schedule]
api_key = "${YANDEX_SCHEDULES_API_KEY}"  # Yandex Schedules API key
from_station = "s9600681"          # Default departure station ID
to_station = "s2000002"            # Default destination station ID
max_results = 3                    # Maximum schedule results
request_timeout = 10               # API request timeout in seconds

[intent_system.handlers.timer]
min_seconds = 1                    # Minimum timer duration in seconds
max_seconds = 86400                # Maximum timer duration in seconds (24 hours)

[intent_system.handlers.random_handler]
default_max_number = 100           # Default maximum for random numbers
max_range_size = 1000000           # Maximum allowed range size
default_dice_sides = 6             # Default number of dice sides

[intent_system.handlers.datetime]
timezone = ""                      # Default timezone (empty = system timezone)
date_format = "%Y-%m-%d"           # Default date format
time_format = "%H:%M:%S"           # Default time format

[intent_system.handlers.greetings]
personalized = true                # Use personalized greetings
context_aware = true               # Consider time of day for greetings

[intent_system.handlers.system]
allow_shutdown = false             # Allow system shutdown commands
allow_restart = false              # Allow system restart commands
info_detail_level = "basic"        # Level of system info to provide (basic/detailed)

# ============================================================
# VAD (VOICE ACTIVITY DETECTION) COMPONENT CONFIGURATION
# ============================================================
[vad]
enabled = true                     # Enable VAD processing to solve 23ms chunk problem
energy_threshold = 0.0015          # Balanced threshold - not too sensitive, not too high (0.0-1.0)
sensitivity = 2.0                  # Higher sensitivity = much lower effective threshold for normal speech (0.1-3.0)
voice_duration_ms = 80             # Shorter minimum duration to catch quick phonemes like "Ð˜", "Ðš" (10-1000)
silence_duration_ms = 400          # Even shorter silence for faster VAD response to speech onset (50-2000)
max_segment_duration_s = 8         # Maximum voice segment duration in seconds (1-60)
use_zero_crossing_rate = true      # Enable ZCR with improved range-based logic
adaptive_threshold = true          # Enable improved adaptive threshold with proper noise estimation
noise_percentile = 20              # Lower percentile for quieter environments - more sensitive (1-50)
voice_multiplier = 2.5             # Reduced multiplier for better sensitivity to normal speech levels (1.0-10.0)
processing_timeout_ms = 50         # Maximum processing time per frame in milliseconds (1-100)
buffer_size_frames = 300           # Maximum frames to buffer in voice segments (10-200)

# Audio normalization for ASR (prevents clipping when VAD triggers on loud audio)
normalize_for_asr = true           # RE-ENABLED - fixed double scaling bug and range issues
asr_target_rms = 0.15              # Target RMS level for ASR audio normalization - increased for better VOSK recognition (0.01-0.3)
enable_fallback_to_original = true # Try original audio if normalized version fails recognition

# Frame-based configuration (internal implementation)
voice_frames_required = 2          # Require 2 consecutive frames for stable detection - prevents false triggers (1-10)
silence_frames_required = 8        # Reduced frames needed to confirm voice end for responsiveness (1-20)

# ============================================================
# MONITORING COMPONENT CONFIGURATION
# ============================================================
[monitoring]
enabled = true                     # Enable unified monitoring system
metrics_enabled = true            # Enable metrics collection
dashboard_enabled = true          # Enable analytics dashboard
notifications_enabled = true     # Enable notification system
debug_tools_enabled = true       # Enable debug tools
memory_management_enabled = true  # Enable memory management

# Notification system configuration
notifications_default_channel = "log"  # Default notification channel: "log", "tts", "web"
notifications_tts_enabled = true       # Enable TTS notifications
notifications_web_enabled = true       # Enable web notifications

# Metrics collection configuration
metrics_monitoring_interval = 300      # Metrics collection interval in seconds
metrics_retention_hours = 24          # Metrics retention period in hours

# Memory management configuration
memory_cleanup_interval = 1800        # Memory cleanup interval in seconds (30 minutes)
memory_aggressive_cleanup = false     # Enable aggressive memory cleanup

# Debug tools configuration
debug_auto_inspect_failures = true    # Automatically inspect failed actions
debug_max_history = 1000              # Maximum debug history entries

# Analytics dashboard configuration
analytics_dashboard_enabled = true    # Enable analytics dashboard web interface
analytics_refresh_interval = 30       # Dashboard refresh interval in seconds

# NOTE: Monitoring endpoints are available through the unified web API at system.web_port
# All monitoring functionality accessible via /monitoring/* endpoints (no separate ports needed)

# ============================================================
# WORKFLOWS - Processing Pipelines (Two-Level Configuration)
# ============================================================
# Two-level hierarchy:
# 1. [components] section controls which components are AVAILABLE system-wide
# 2. [workflows.X] section controls which pipeline stages this workflow USES
# 
# Validation rules:
# - workflow stage can only be enabled if corresponding component is enabled
# - component enabled + workflow stage disabled = WARNING (wasteful but valid)
# - component disabled + workflow stage enabled = FATAL ERROR

[workflows]
enabled = ["unified_voice_assistant"]  # List of enabled workflows
default = "unified_voice_assistant"    # Default workflow to execute

# Unified Voice Assistant Workflow Configuration
[workflows.unified_voice_assistant]
# Pipeline stage enablement (requires corresponding components to be enabled)
voice_trigger_enabled = true       # Enable voice trigger stage (requires components.voice_trigger = true)
asr_enabled = true                 # Enable ASR stage (requires components.asr = true)
text_processing_enabled = true     # Enable text processing stage (requires components.text_processor = true)
nlu_enabled = true                 # Enable NLU stage (requires components.nlu = true)
intent_execution_enabled = true    # Enable intent execution stage (requires components.intent_system = true)
llm_enabled = true                 # Enable LLM processing stage (requires components.llm = true)
tts_enabled = true                 # Enable TTS output stage (requires components.tts = true)
audio_enabled = true               # Enable audio playback stage (requires components.audio = true)
monitoring_enabled = true          # Enable monitoring and metrics stage (requires components.monitoring = true)

# Audio processing configuration
buffer_size_ms = 100.0             # Audio buffer size in milliseconds
sample_rate = 16000                # Audio sample rate for processing
channels = 1                       # Number of audio channels for processing
validate_audio_compatibility = true # Validate cross-component audio compatibility at startup
enable_vad_processing = true       # Enable Voice Activity Detection processing for audio pipeline

# NOTE: Audio flow with automatic resampling:
# - Microphone: Hardware-dependent sample rate (e.g., 44100 Hz)
# - Processing: Components use optimized rates (ASR=16000 Hz, Voice Trigger=16000 Hz)  
# - Resampling: Automatic conversion between microphone input and processing components
# - TTS/Audio: Output-only components can use different rates as needed

# Request context defaults
default_source = "voice"           # Default request source
default_language = "ru"            # Default processing language
session_timeout_minutes = 30       # Session timeout for context

# Audio compatibility and validation
[workflows.unified_voice_assistant.audio_validation]
enabled = true                     # Enable startup audio compatibility validation
fatal_on_conflicts = true         # Fatal error when configuration contradicts provider requirements  
auto_resolve_defaults = true      # Use provider defaults when configuration is unspecified
log_resolution_details = true     # Log audio configuration resolution process

# Global resampling defaults (can be overridden per component)
[workflows.unified_voice_assistant.resampling]
default_quality = "medium"        # Default resampling quality: "fast", "medium", "high", "best"
cache_conversions = true          # Cache resampling conversions for performance
max_cache_size_mb = 50            # Maximum resampling cache size in megabytes
performance_monitoring = true     # Monitor resampling performance and latency

# ============================================================
# ASSET MANAGEMENT - Comprehensive Configuration
# ============================================================
[assets]
# Directory management
auto_create_dirs = true            # Automatically create asset directories
cleanup_on_startup = false         # Clean temporary files on startup

# Download configuration  
auto_download = true               # Automatically download missing models
download_timeout_seconds = 300     # Download timeout in seconds (5 minutes)
max_download_retries = 3           # Maximum download retry attempts
verify_downloads = true            # Verify downloaded file integrity

# Cache configuration
cache_enabled = true               # Enable model and file caching
max_cache_size_mb = 2048           # Maximum cache size in megabytes (2GB)
cache_ttl_hours = 24               # Cache time-to-live in hours

# Model management
preload_essential_models = false   # Preload essential models on startup (requires more memory)
model_compression = true           # Use compressed model formats when available
concurrent_downloads = 2           # Maximum concurrent model downloads

# Asset paths are controlled by environment variables:
# IRENE_ASSETS_ROOT (default: ~/.cache/irene)
#   â”œâ”€â”€ models/          - AI models and weights (automatically managed)
#   â”œâ”€â”€ cache/           - Runtime cache and temporary files
#   â””â”€â”€ credentials/     - API keys and authentication files
#
# NOTE: Model paths (model_path, model_paths) are NOT configurable in provider settings.
# All AI models are automatically managed by the Asset Manager system:
# - Models are downloaded to IRENE_ASSETS_ROOT/models/{provider}/{model_id}
# - Providers automatically discover their models via asset manager
# - No manual path configuration needed - just enable the provider!

# Environment variable examples:
# export IRENE_ASSETS_ROOT="/opt/irene/assets"
# export ELEVENLABS_API_KEY="your_api_key_here"
# export OPENAI_API_KEY="your_openai_key_here"
# export ANTHROPIC_API_KEY="your_anthropic_key_here"
# export GOOGLE_APPLICATION_CREDENTIALS="/path/to/service-account.json"
# export PICOVOICE_ACCESS_KEY="your_picovoice_key_here"
# export YANDEX_SCHEDULES_API_KEY="your_yandex_key_here"
#
# Model preloading examples (override TOML settings):
# export IRENE_TTS__PROVIDERS__SILERO_V4__PRELOAD_MODELS=true
# export IRENE_ASR__PROVIDERS__WHISPER__PRELOAD_MODELS=true
# export IRENE_ASR__PROVIDERS__VOSK__PRELOAD_MODELS=true
# export IRENE_VOICE_TRIGGER__PROVIDERS__OPENWAKEWORD__PRELOAD_MODELS=true

# ============================================================
# EXAMPLE DEPLOYMENT PROFILES
# ============================================================

# To create specific deployment profiles, copy relevant sections:

# VOICE ASSISTANT PROFILE (full functionality):
# - Enable all components: tts=true, audio=true, asr=true, voice_trigger=true
# - Enable microphone: system.microphone_enabled=true, inputs.microphone=true
# - Configure providers with API keys
# - Enable model preloading for faster response: preload_models=true for key providers
# - Audio config: Set consistent sample_rate across microphone/asr/voice_trigger (e.g., 16000)
# - Enable resampling: allow_resampling=true with appropriate quality levels per component

# API-ONLY PROFILE (text processing only):
# - Disable voice components: tts=false, audio=false, asr=false, voice_trigger=false
# - Enable text processing: llm=true, nlu=true, text_processor=true
# - Enable web API: system.web_api_enabled=true, inputs.web=true
# - Consider preloading NLU models: nlu.providers.spacy_nlu.preload_models=true

# HEADLESS PROFILE (minimal functionality):
# - Disable all audio: system.microphone_enabled=false, system.audio_playback_enabled=false
# - Disable web API: system.web_api_enabled=false
# - Enable CLI only: inputs.cli=true, inputs.default_input="cli"
# - Disable model preloading to save memory: preload_models=false for all providers

# EMBEDDED PROFILE (resource-constrained):
# - Use lightweight providers: asr.providers.vosk, tts.providers.console
# - Disable resource-intensive features: llm=false, text_processor.providers.tts_text_processor=false
# - Reduce concurrency: max_concurrent_commands=3
# - Disable model preloading: preload_models=false for all providers
# - Reduce cache sizes: assets.max_cache_size_mb=256
# - Audio optimization: Use "fast" resampling quality, reduce resampling cache size
# - Disable performance monitoring: workflows.unified_voice_assistant.resampling.performance_monitoring=false

# ============================================================
# END OF MASTER CONFIGURATION
# ============================================================
