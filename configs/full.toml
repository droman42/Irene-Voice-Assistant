# ============================================================
# IRENE VOICE ASSISTANT v14.0.0 - CLEAN ARCHITECTURE
# ============================================================

name = "Irene"
version = "15.0.0"
debug = false
log_level = "INFO"

# ============================================================
# SYSTEM CAPABILITIES - Hardware & Services
# ============================================================
[system]
microphone_enabled = false      # Hardware capability
audio_playback_enabled = false  # Hardware capability
web_api_enabled = true         # Service capability
web_port = 8000
metrics_enabled = false
metrics_port = 9090

# ============================================================
# INPUT SOURCES - Data Entry Points
# ============================================================
[inputs]
microphone = false              # Microphone input source
web = true                     # Web interface input
cli = true                     # Command line input
default_input = "web"

# Microphone Input Configuration
[inputs.microphone_config]
enabled = true
# device_id = null   # Uncomment and set to specify audio device (null = default device)
sample_rate = 16000      # Audio sample rate (Hz)
channels = 1            # 1 = mono, 2 = stereo
chunk_size = 1024        # Audio buffer size

# Web Input Configuration
[inputs.web_config]
enabled = true
websocket_enabled = true
rest_api_enabled = true

# CLI Input Configuration
[inputs.cli_config]
enabled = true
prompt_prefix = "irene> "
history_enabled = true

# ============================================================
# COMPONENT CONFIGURATIONS - Provider Management
# ============================================================
[tts]
enabled = true
default_provider = "console"
fallback_providers = ['console']


[audio]
enabled = true
default_provider = "console"
fallback_providers = ['console']
concurrent_playback = false


[asr]
enabled = true
default_provider = "whisper"
fallback_providers = ['whisper']


[llm]
enabled = true
default_provider = "openai"
fallback_providers = ['anthropic', 'vsegpt']


[voice_trigger]
enabled = true
default_provider = "openwakeword"
wake_words = ['irene', 'jarvis']
confidence_threshold = 0.8
buffer_seconds = 1.0
timeout_seconds = 5.0


[nlu]
enabled = true
default_provider = "hybrid_keyword_matcher"
confidence_threshold = 0.7
fallback_intent = "conversation.general"
provider_cascade_order = ['hybrid_keyword_matcher', 'spacy_nlu']
max_cascade_attempts = 4
cascade_timeout_ms = 200
cache_recognition_results = false
cache_ttl_seconds = 300

# Language Detection Configuration
auto_detect_language = true
language_detection_confidence_threshold = 0.8
persist_language_preference = true
supported_languages = ["ru", "en"]
default_language = "ru"

# Hybrid Keyword Matcher Provider (fast, pattern-based) - Phase 1 Enhanced
[nlu.providers.hybrid_keyword_matcher]
enabled = true
confidence_threshold = 0.7
fuzzy_enabled = true
fuzzy_threshold = 0.8
pattern_confidence = 0.9
case_sensitive = false
normalize_unicode = true
cache_fuzzy_results = true
max_fuzzy_keywords_per_intent = 50
min_pattern_length = 2

# SpaCy NLU Provider (advanced linguistic analysis) - Phase 1 Multi-Model Support
[nlu.providers.spacy_nlu]
enabled = true
confidence_threshold = 0.7
entity_types = ["PERSON", "ORG", "GPE", "DATE", "TIME", "MONEY", "QUANTITY"]

# Phase 1: Multi-model language preferences
[nlu.providers.spacy_nlu.language_preferences]
ru = ["ru_core_news_md", "ru_core_news_sm"]
en = ["en_core_web_md", "en_core_web_sm"]


[text_processor]
enabled = true
stages = ['asr_output', 'tts_input', 'command_input', 'general']


# ============================================================
# VAD CONFIGURATION - Voice Activity Detection
# ============================================================
[vad]
enabled = true                # Enable VAD to solve 23ms chunk problem
energy_threshold = 0.01       # RMS energy threshold for voice detection
sensitivity = 0.5             # Detection sensitivity multiplier
voice_duration_ms = 100       # Minimum voice duration in milliseconds
silence_duration_ms = 200     # Minimum silence duration to end voice segment
max_segment_duration_s = 10   # Maximum voice segment duration in seconds
use_zero_crossing_rate = true # Enable Zero Crossing Rate analysis
adaptive_threshold = false    # Disable for consistent testing


# ============================================================
# WORKFLOWS - Processing Pipelines
# ============================================================
[workflows]
enabled = ['unified_voice_assistant']
default = "unified_voice_assistant"

[workflows.unified_voice_assistant]
enable_vad_processing = true  # Enable VAD processing to solve chunk problem

# ============================================================
# ASSET MANAGEMENT - Environment-Driven
# ============================================================
[assets]
auto_create_dirs = true
# Paths use environment variable defaults:
# IRENE_ASSETS_ROOT (default: ~/.cache/irene)

# Language and locale
language = "en-US"

# Runtime settings
max_concurrent_commands = 10
command_timeout_seconds = 30.0
context_timeout_minutes = 30
